FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad
MODULE: CLASSIFICATION-1
LAB-3 : Using KNN for Text Classification
Module Coordinator: Jashn Arora
Section 1: Understanding NLP tools
In this lab we will be using KNN on a real world NLP application i.e. is text classification. But first look at some NLP techniques for text classification and tools that we use when we want to use python for NLP.

Section 1.2: Data Cleaning and Preprocessing step
Raw text must be processed and converted into a form so that it is suitable to use with various machine-learning algorithms.
In case of text, there are lots of things that need to be taken into account.

Removing numbers from the text
Handling capitalization and punctuation.
Stemming and Lemmatizing text.
And most importantly, one can't just use words or images directly in algorithms; they need to be converted into vectors- a form that algorithms can understand.

NLTK
NLTK (or Natural Language Tool Kit) is a commonly used library for processing text. We will use this tool in this lab. Lets first install it.


[ ]
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Downloading package omw-1.4 to /root/nltk_data...
True

[ ]
import re
import numpy
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import wordnet
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from bs4 import BeautifulSoup

def cleanText(text, lemmatize, stemmer):
    """Method for cleaning text from train and test data. Removes numbers, punctuation, and capitalization. Stems or lemmatizes text."""

    if isinstance(text, float):
        text = str(text)
    if isinstance(text, numpy.int64):
        text = str(text)
    try:
        text = text.decode()
    except AttributeError:
        pass

    soup = BeautifulSoup(text, "lxml")
    text = soup.get_text()
    text = re.sub(r"[^A-Za-z]", " ", text)
    text = text.lower()


    if lemmatize:
        wordnet_lemmatizer = WordNetLemmatizer()

        def get_tag(tag):
            if tag.startswith('J'):
                return wordnet.ADJ
            elif tag.startswith('V'):
                return wordnet.VERB
            elif tag.startswith('N'):
                return wordnet.NOUN
            elif tag.startswith('R'):
                return wordnet.ADV
            else:
                return ''

        text_result = []
        tokens = word_tokenize(text)  # Generate list of tokens
        tagged = pos_tag(tokens)
        for t in tagged:
            try:
                text_result.append(wordnet_lemmatizer.lemmatize(t[0], get_tag(t[1][:2])))
            except:
                text_result.append(wordnet_lemmatizer.lemmatize(t[0]))
        return text_result

    if stemmer:
        text_result = []
        tokens = word_tokenize(text)
        snowball_stemmer = SnowballStemmer('english')
        for t in tokens:
            text_result.append(snowball_stemmer.stem(t))
        return text_result

[ ]
sample_text = "Troubling"
sample_text_result = cleanText(sample_text, lemmatize=False, stemmer=True)
sample_text_result = " ".join(str(x) for x in sample_text_result)
print(sample_text)
print(sample_text_result)
sample_text_result = cleanText(sample_text, lemmatize=True, stemmer=False)
sample_text_result = " ".join(str(x) for x in sample_text_result)
print(sample_text_result)
Troubling
troubl
trouble
Section 1.2: BAG OF WORDS
A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.

The approach is very simple and flexible, and can be used in many ways for extracting features from documents.

A bag-of-words is a representation of text that describes the occurrence of words within a document. It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.


[ ]
# Functions to convert document(s) to a list of words, with the option of removing stopwords. Returns document-term matrix.

def createBagOfWords(train, test, remove_stopwords, lemmatize, stemmer):
    if remove_stopwords:
        vectorizer = CountVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))
    else:
        vectorizer = CountVectorizer(analyzer='word', input='content')

    clean_train = []
    for paragraph in train:
        paragraph_result = cleanText(paragraph, lemmatize, stemmer)
        paragraph = " ".join(str(x) for x in paragraph_result)
        clean_train.append(paragraph)

    clean_test = []
    for paragraph in test:
        paragraph_result = cleanText(paragraph, lemmatize, stemmer)
        paragraph = " ".join(str(x) for x in paragraph_result)
        clean_test.append(paragraph)

    bag_of_words_train = vectorizer.fit_transform(clean_train).toarray()
    bag_of_words_test = vectorizer.transform(clean_test).toarray()
    return bag_of_words_train, bag_of_words_test

Section 1.3: TF-IDF
TF-IDF technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers.

The number of times a term occurs in a document is called its Term frequency (TF).

Document frequency is the number of documents in which the word is present. Inverse DF (IDF) is the inverse of the document frequency which measures the informativeness of term t.


[ ]
def createTFIDF(train, test, remove_stopwords, lemmatize, stemmer):
    if remove_stopwords:
        vectorizer = TfidfVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))
    else:
        vectorizer =  TfidfVectorizer(analyzer='word', input='content')

    clean_train = []
    for paragraph in train:
        paragraph_result = cleanText(paragraph, lemmatize, stemmer)
        paragraph = " ".join(str(x) for x in paragraph_result)
        clean_train.append(paragraph)

    clean_test = []
    for paragraph in test:
        paragraph_result = cleanText(paragraph, lemmatize, stemmer)
        paragraph = " ".join(str(x) for x in paragraph_result)
        clean_test.append(paragraph)

    tfidf_train = vectorizer.fit_transform(clean_train).toarray()
    tfidf_test = vectorizer.transform(clean_test).toarray()
    return tfidf_train, tfidf_test
Section 2: UNDERSTANDING THE DATA : A REVIEWS DATASET
Sentiment analysis is the interpretation and classification of emotions (such as positive, negative and neutral) within text data using text analysis techniques.
Given below is a dataset consisting of reviews along with sentiment class (positive or negative).


[ ]
# Upload the Reviews CSV file that has been shared with you.
# Run this cell, click on the 'Choose files' button and upload the file.
from google.colab import files
uploaded = files.upload()


[ ]
import pandas as pd
df = pd.read_csv('reviews.csv')

[ ]
df = df.dropna()

[ ]
df.to_csv('reviews.csv', index=False)
Section 3: KNN MODEL
Given below are two KNN models; in the first case we are using Bag-of-Words and in the second case we are using TF-IDF. Note the different metrics and parameters used in each.


[ ]
from sklearn import metrics, neighbors
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict

## TASK - 1: Tweak the models below and see results with different parameters and distance metrics.

def bow_knn():
    """Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm"""

    training_data = pd.read_csv('reviews.csv')
    X_train, X_test, y_train, y_test = train_test_split(training_data["sentence"], training_data["sentiment"], test_size=0.2, random_state=5)
    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)
    # print(X_train)
    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)

    knn.fit(X_train, y_train)
    predicted = knn.predict(X_test)
    acc = metrics.accuracy_score(y_test, predicted)
    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')

    scores = cross_val_score(knn, X_train, y_train, cv=3)
    print("Cross Validation Accuracy: %0.2f" % (scores.mean()))
    print(scores)
    print('\n')
    return predicted, y_test


def tfidf_knn():
    """Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm"""

    training_data = pd.read_csv('reviews.csv')
    X_train, X_test, y_train, y_test = train_test_split(training_data["sentence"], training_data["sentiment"],
                                                        test_size=0.2, random_state=5)
    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)
    # print(X_train)
    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2,
                                         metric='cosine', metric_params=None, n_jobs=1)

    knn.fit(X_train, y_train)
    predicted = knn.predict(X_test)
    acc = metrics.accuracy_score(y_test, predicted)
    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')

    scores = cross_val_score(knn, X_train, y_train, cv=3)
    print("Cross Validation Accuracy: %0.2f" % (scores.mean()))
    print(scores)
    return predicted, y_test
Note: Cross-validation will be discussed in detail in the upcoming lab session.


[ ]
## KNN accuracy after using BoW
predicted, y_test = bow_knn()
<ipython-input-2-3fc32437a98d>:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.
  soup = BeautifulSoup(text, "lxml")
KNN with BOW accuracy = 62.30366492146597%
/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.
  warnings.warn(
Cross Validation Accuracy: 0.62
[0.60784314 0.58431373 0.66141732]



[ ]
## KNN accuracy after using TFIDF
predicted, y_test = tfidf_knn()
<ipython-input-2-3fc32437a98d>:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.
  soup = BeautifulSoup(text, "lxml")
KNN with TFIDF accuracy = 70.15706806282722%
/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.
  warnings.warn(
Cross Validation Accuracy: 0.73
[0.7254902  0.74117647 0.72834646]
Section 4: SPAM TEXT DATASET
Now let's use what we've learnt to classify texts as spam or not spam.


[ ]
# Upload the spam text data CSV file that has been shared with you. You can also download the file from https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset
# Run this cell, click on the 'Choose files' button and upload the file.
from google.colab import files
uploaded = files.upload()


[ ]
import pandas as pd
df = pd.read_csv('spam.csv')
df


[ ]
df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})

[ ]
df.head(5)


[ ]
len(df)
5572

[ ]
from sklearn import metrics, neighbors
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict

## TASK - 2: Tweak the models below and see results with different parameters and distance metrics.

def bow_knn():
    """Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm"""

    training_data = pd.read_csv('spam.csv')
    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})
    X_train, X_test, y_train, y_test = train_test_split(training_data["Message"], training_data["Category"], test_size=0.2, random_state=5)
    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)
    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)

    knn.fit(X_train, y_train)
    predicted = knn.predict(X_test)
    acc = metrics.accuracy_score(y_test, predicted)
    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')

    scores = cross_val_score(knn, X_train, y_train, cv=3)
    print("Cross Validation Accuracy: %0.2f" % (scores.mean()))
    print(scores)
    print('\n')
    return predicted, y_test


def tfidf_knn():
    """Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm"""

    training_data = pd.read_csv('spam.csv')
    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})
    X_train, X_test, y_train, y_test = train_test_split(training_data["Message"], training_data["Category"], test_size=0.2, random_state=5)
    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)
    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2, metric='cosine', metric_params=None, n_jobs=1)

    knn.fit(X_train, y_train)
    predicted = knn.predict(X_test)
    acc = metrics.accuracy_score(y_test, predicted)
    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')

    scores = cross_val_score(knn, X_train, y_train, cv=3)
    print("Cross Validation Accuracy: %0.2f" % (scores.mean()))
    print(scores)
    return predicted, y_test

[ ]
# This cell may take some time to run
predicted, y_test = bow_knn()
<ipython-input-2-3fc32437a98d>:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.
  soup = BeautifulSoup(text, "lxml")
KNN with BOW accuracy = 92.19730941704036%
Cross Validation Accuracy: 0.91
[0.90713324 0.90040377 0.91245791]



[ ]
# This cell may take some time to run
predicted, y_test = tfidf_knn()
<ipython-input-2-3fc32437a98d>:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.
  soup = BeautifulSoup(text, "lxml")
KNN with TFIDF accuracy = 98.56502242152466%
Cross Validation Accuracy: 0.97
[0.96837147 0.96769852 0.96363636]
Questions to Think About and Answer
Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?
The **TF-IDF (Term Frequency-Inverse Document Frequency)** approach often achieves better accuracy than the **Bag-of-Words (BoW)** model because it addresses several limitations of BoW by assigning weights to words based on their importance in a collection of documents. Here's why TF-IDF usually performs better:

---

### **1. Reduces the Impact of Common Words**
   - **Bag-of-Words**: Treats all words equally, so high-frequency common words (e.g., "the," "and") dominate the representation, even though they may not carry much meaningful information.
   - **TF-IDF**: Assigns lower weights to common words through the **IDF (Inverse Document Frequency)** component, making the representation focus more on distinctive and informative terms.

---

### **2. Highlights Important Words**
   - **Bag-of-Words**: Only counts word frequencies, so all frequent words are treated equally, regardless of their contextual importance.
   - **TF-IDF**: Emphasizes words that appear frequently in a specific document but rarely across the entire corpus. This makes it better at capturing the unique characteristics of each document.

---

### **3. Reduces Noise from Irrelevant Features**
   - **Bag-of-Words**: Includes all words as features, which can introduce noise from unimportant or redundant terms, potentially harming model performance.
   - **TF-IDF**: Weighs terms based on their relevance, reducing the influence of irrelevant or redundant features, and leading to a more meaningful representation.

---

### **4. Better Differentiation Between Documents**
   - **Bag-of-Words**: May struggle to differentiate between documents that share many common words but differ in key terms.
   - **TF-IDF**: Captures these differences more effectively by weighting unique terms higher, improving the ability to distinguish between similar documents.

---

### **5. More Effective for Sparse Representations**
   - Text data is inherently sparse, as most words appear in only a few documents. TF-IDF is designed to work well with sparse matrices by balancing term frequency (TF) with global importance (IDF).

---

### Example:
Consider two documents:
- Document 1: "The cat sat on the mat."
- Document 2: "The cat sleeps on the bed."

- **Bag-of-Words Representation**:
   - Both documents might look very similar because the model only considers word counts.
   - Common terms like "the" and "on" will dominate, making it harder to differentiate between the two.

- **TF-IDF Representation**:
   - Words like "mat" and "bed," which are unique to each document, will receive higher weights due to their lower frequency in the corpus.
   - Common terms like "the" and "on" will have lower weights, allowing the model to focus on the distinguishing features.

---

### **When TF-IDF Excels**
TF-IDF generally excels in scenarios where:
- The corpus has a mix of common and unique terms.
- Text classification, information retrieval, and clustering tasks require identifying documents' distinguishing features.

However, **TF-IDF may not perform as well** in cases where:
- Document frequency (IDF) is not meaningful (e.g., very small corpora).
- Word sequence and context are crucial (e.g., sentiment analysis, where word order matters).

For tasks that depend heavily on context or semantics, advanced approaches like **word embeddings** (e.g., Word2Vec, GloVe, BERT) might outperform TF-IDF.
Can you think of techniques that are better than both BoW and TF-IDF ?
Yes, several techniques surpass **Bag-of-Words (BoW)** and **TF-IDF** by addressing their limitations, particularly their inability to capture context, word order, and semantic relationships between words. These advanced techniques leverage more sophisticated representations and often perform better in text analysis tasks. Here are some notable ones:

---

### **1. Word Embeddings**
   Word embeddings represent words in continuous, dense vector spaces where similar words have similar representations. Unlike BoW and TF-IDF, they capture semantic relationships and word meanings.

   #### Examples:
   - **Word2Vec** (Skip-Gram and CBOW):
     - Learns word embeddings based on the context in which words appear.
     - Captures relationships like "king - man + woman = queen."
   - **GloVe (Global Vectors for Word Representation):**
     - Combines local context with global word co-occurrence statistics.
   - **FastText**:
     - Enhances embeddings by considering subword information, which helps with rare and out-of-vocabulary words.

   #### Advantages:
   - Dense, lower-dimensional representations (compared to sparse vectors in BoW/TF-IDF).
   - Captures semantic and syntactic relationships.
   - Context-aware at a basic level (using local word windows).

---

### **2. Contextualized Word Embeddings**
   These techniques generate dynamic word embeddings that depend on the surrounding context, addressing the limitations of static embeddings like Word2Vec.

   #### Examples:
   - **ELMo (Embeddings from Language Models):**
     - Generates context-sensitive embeddings by analyzing entire sentences.
   - **BERT (Bidirectional Encoder Representations from Transformers):**
     - Uses transformers to produce embeddings for each word based on the full context (both preceding and following words).
   - **GPT (Generative Pre-trained Transformers):**
     - Focuses on unidirectional or bidirectional context depending on the model version.

   #### Advantages:
   - Handles polysemy (words with multiple meanings) effectively (e.g., "bank" in "river bank" vs. "financial bank").
   - Encodes rich contextual and semantic information.
   - Provides sentence-level embeddings or token-level embeddings as needed.

---

### **3. Sentence and Document Embeddings**
   These methods produce embeddings for entire sentences, paragraphs, or documents, capturing relationships at higher levels than individual words.

   #### Examples:
   - **Doc2Vec (Paragraph Vectors):**
     - Extends Word2Vec to create embeddings for entire documents.
   - **Universal Sentence Encoder (USE):**
     - Encodes sentences into fixed-length vectors for tasks like classification or similarity.
   - **InferSent**:
     - Focuses on generating sentence embeddings that perform well on downstream tasks like entailment or sentiment analysis.

   #### Advantages:
   - Encodes the meaning of entire texts, not just individual words.
   - Reduces the dimensionality of representations compared to BoW/TF-IDF.

---

### **4. Transformer-Based Language Models**
   Transformers have revolutionized NLP by learning complex relationships between words and context.

   #### Examples:
   - **BERT**:
     - Pre-trained on large corpora; fine-tuned for specific tasks.
   - **RoBERTa, DistilBERT, and ALBERT**:
     - Variants of BERT optimized for efficiency or performance.
   - **T5 and GPT Models**:
     - Models that focus on text generation or transformation tasks.

   #### Advantages:
   - Superior performance across a wide range of NLP tasks.
   - Captures both context and complex relationships between words and phrases.
   - State-of-the-art results in tasks like sentiment analysis, question answering, and machine translation.

---

### **5. Graph-Based Representations**
   Graph-based techniques represent words, sentences, or documents as nodes in a graph, with edges encoding relationships.

   #### Example:
   - **Graph Neural Networks (GNNs)** for text:
     - Capture relationships between words, entities, or entire documents.
     - Useful for tasks like document classification or entity recognition.
   - **TextRank** (for summarization or keyword extraction):
     - Ranks words or phrases in a graph structure based on their co-occurrence.

   #### Advantages:
   - Captures relationships beyond simple word proximity.
   - Effective for tasks involving structured relationships (e.g., summarization, entity linking).

---

### **6. Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)**
   #### Description:
   - **LSA**: Uses Singular Value Decomposition (SVD) to reduce the dimensionality of term-document matrices and uncover latent topics.
   - **LDA**: A probabilistic model that assumes documents are a mixture of latent topics, each represented by a distribution over words.

   #### Advantages:
   - Identifies latent topics in text.
   - Captures some semantic structure in a corpus.

---

### **Comparison of Advanced Techniques with BoW/TF-IDF**
| **Technique**              | **Captures Context** | **Semantic Meaning** | **Dimensionality**   | **Scalability**       |
|----------------------------|---------------------|----------------------|----------------------|-----------------------|
| **BoW**                   | No                  | No                   | High (Sparse)        | High (Simple Model)   |
| **TF-IDF**                | No                  | Partial (Weights)    | High (Sparse)        | High                  |
| **Word2Vec/GloVe**        | Local Context       | Yes                  | Low (Dense)          | Medium                |
| **BERT/Transformers**     | Full Context        | Yes                  | Low (Dense)          | Low (Computationally Expensive) |
| **Doc2Vec**               | Sentence/Document   | Yes                  | Low (Dense)          | Medium                |

In summary, while BoW and TF-IDF are foundational and efficient for small-scale tasks, more advanced techniques like **word embeddings**, **contextualized embeddings**, and **transformer models** provide superior performance by leveraging richer semantic and contextual information.
Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each.
Useful Resources for further reading
Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html
TF-IDF and BoW : https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/
TF-IDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html

[ ]

Start coding or generate with AI.
